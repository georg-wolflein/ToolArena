This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  workflows/
    docker.yml
docker/
  function_runner.py
  runtime.Dockerfile
  subprocess_utils.py
  tool.Dockerfile
tasks/
  conch_extract_features/
    implementation.py
    install.sh
    task.yaml
    tests.py
  stamp_extract_features/
    data/
      download.sh
      gdc_manifest.brca.txt
      gdc_manifest.crc.txt
    .gitignore
  .gitignore
  conftest.py
  pytest.ini
  utils.py
toolarena/
  __init__.py
  __main__.py
  definition.py
  run.py
  runtime.py
  server.py
  utils.py
.cursorignore
.dockerignore
.gitignore
.python-version
.repomixignore
pyproject.toml
run_tool.ipynb
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".repomixignore">
.legacy/
/runs/
*.xlsx
*.csv
*.svs
/papers
/data
</file>

<file path="docker/function_runner.py">
"""This is a standalone, dependency-free script that can be used to run a function.

Usage:
python function_runner.py <info_path>

Where <info_path> is the path to the function info file. The file should be a JSON file with the following structure:
{
    "path": <path_to_function_file>,      # Path to the Python file containing the function
    "name": <name_of_function>,           # The name of the function to call
    "args": <arguments_of_function>,      # The keyword arguments to pass to the function, as a dictionary
    "output_path": <path_to_output_file>  # The path to the output file. The function must produce a JSON serializable object.
}
"""

import argparse
import importlib.util
import json
import os
from pathlib import Path
from typing import Any


def load_symbol_from_file(file_path: os.PathLike, symbol_name: str) -> Any:
    """Load a symbol from a file."""
    spec = importlib.util.spec_from_file_location("module.name", str(file_path))
    module = importlib.util.module_from_spec(spec) if spec else None
    if not spec or not spec.loader or not module:
        raise ValueError(f"Failed to load module from {file_path}")
    spec.loader.exec_module(module)
    func = getattr(module, symbol_name, None)
    if not func:
        raise ValueError(f"Symbol {symbol_name} not found in {file_path}")
    return func


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run a function.")
    parser.add_argument(
        "info_path", type=Path, help="The path to the function info file."
    )
    args = parser.parse_args()

    with args.info_path.open("r") as f:
        info = json.load(f)
    function = load_symbol_from_file(info["path"], info["name"])
    result = function(**info["args"])
    with open(info["output_path"], "w") as f:
        json.dump({"result": result}, f)
</file>

<file path="docker/subprocess_utils.py">
import os
import subprocess
from collections.abc import Mapping


def run_and_stream_command(
    args,
    *,
    bufsize: int = 16,
    stdin=None,
    stdout=None,
    stderr=None,
    shell=True,
    text=False,
    env: Mapping[str, str] = {},
    start_new_session=True,
    **kwargs,
):
    assert not text, "text=True is not supported"
    assert start_new_session, "start_new_session=False is not supported"

    output = bytearray()
    buffer = bytearray()

    # Start the subprocess
    with subprocess.Popen(
        args,
        **kwargs,
        stdin=subprocess.DEVNULL,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        bufsize=16,
        text=False,
        shell=shell,
        env={**dict(os.environ), **env},
        start_new_session=True,
    ) as process:  # type: ignore
        try:
            # Stream combined stdout and stderr
            while True:
                chunk = process.stdout.read(32)
                if not chunk:
                    break
                output.extend(chunk)
                buffer.extend(chunk)

                try:
                    # Decode and print valid UTF-8 sequences
                    text = buffer.decode("utf-8")
                    print(text, end="")
                    buffer.clear()  # Clear buffer if decoding was successful
                except UnicodeDecodeError:
                    # Keep partial data in the buffer
                    pass
        except Exception:
            process.kill()
            raise
    # Wait for the process to finish and get the return code
    return_code = process.wait()

    # Decode any remaining data in the buffer
    decoded_output = output.decode("utf-8")
    if (lines := decoded_output.strip().splitlines()) and lines[-1].startswith(
        "EOFError"
    ):
        decoded_output += (
            "\nNOTE: The EOFError may be caused because the script is waiting for user input, "
            "which is not supported. Make sure that you do not run commands that require user input!\n"
        )
    return return_code, decoded_output
</file>

<file path="tasks/stamp_extract_features/data/download.sh">
#!/bin/bash
set -e

for manifest in gdc_manifest.brca.txt gdc_manifest.crc.txt; do
    uvx --from git+https://github.com/NCI-GDC/gdc-client gdc-client download --manifest $manifest
    mv */*.svs .
    rm -rf */logs
done
</file>

<file path="tasks/stamp_extract_features/data/gdc_manifest.brca.txt">
id	filename	md5	size	state
d15f6213-f780-4794-8ad3-ad9fd635f9b2	TCGA-BH-A0HQ-01Z-00-DX1.0921FCEF-20A2-4D4B-A198-91AF9F6C814C.svs	aaec48746dbbe81579c75e8b59438d65	1162280135	released
8883da25-f7d7-4501-bb14-6b9a954721b6	TCGA-B6-A0IJ-01Z-00-DX1.BF2E062F-06DA-4CA8-86C4-36674C035CAA.svs	9b707c90a918d3897e60c7e7b5b19990	300015753	released
1ddaf37b-31c3-4e1c-86b4-8927fe67c3d8	TCGA-BH-A0C3-01Z-00-DX1.14D3210D-0CBE-4DCA-A986-A26AE5382502.svs	3fa7a44e12cb296af47fee83d604b68c	1082867913	released
a6126421-0b08-47f8-bd5e-3147ba8c9f96	TCGA-PL-A8LX-01A-01-DX1.9646D69F-A764-4246-9243-67A63006DE96.svs	1c29c60db1cd7dce01f386e9c632e872	181293480	released
8a6b8d0f-2fac-47f6-9973-10cbb338e139	TCGA-PL-A8LZ-01A-03-DX3.E5D16DBF-CABD-4C96-A794-5F27BC305055.svs	9be6ba6d0d167a9b7052553441ea9e41	347265102	released
cb3ca443-0348-4a33-90d9-1e1c914efe2a	TCGA-BH-A0BZ-01Z-00-DX1.45EB3E93-A871-49C6-9EAE-90D98AE01913.svs	a4f047e9ba19a78ab79f23558ac95873	1173967467	released
acf61f87-1363-4abb-a435-4d1c78e9a912	TCGA-PL-A8LV-01A-02-DX2.346FF42B-2363-4B37-8884-12D5F841842C.svs	76b6978a36c5c24120b5798f171bd11f	244034918	released
f935cbf5-2afc-4baa-8bf8-c77c0c7d76ed	TCGA-A2-A04Y-01Z-00-DX1.4DC97AD6-4806-4A3A-A998-FD36F93590A4.svs	ab9247aa60430c29eecd9070a7ca2158	534447883	released
c8c3d75b-78e5-47ab-9d97-ef0706e44ed0	TCGA-PL-A8LY-01A-01-DX1.32047D5E-8A42-480A-B2B8-A56B47B949FD.svs	57b0977907f3b8f314273219bc0dea71	161766610	released
</file>

<file path="tasks/stamp_extract_features/data/gdc_manifest.crc.txt">
id	filename	md5	size	state
001b7d97-9425-43c3-a9a3-a36cb3d2a591	TCGA-A6-2686-01Z-00-DX1.0540a027-2a0c-46c7-9af0-7b8672631de7.svs	180d8f24d037b1e5e70dcd3dbc360b2d	497633503	released
31565365-1404-46ff-b4f6-a5c5294c9ba6	TCGA-A6-5661-01Z-00-DX1.bad2d858-11b4-4b9c-a720-daaae592cf48.svs	f53e6c9afcad8e8d3dee383e8bff8c79	168764181	released
dffdc56e-f1f5-4a1b-a16b-561462b3b740	TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B.svs	101243b373ac1cb67b393c510872855b	938870973	released
0b55ba87-0994-4679-9d1c-6cc3ddfd3078	TCGA-A6-5662-01Z-00-DX1.82569684-1c31-4346-af9b-c296a020f624.svs	4c15ecb676dc05e99167ee33c63a20ed	94426693	released
5ad0faff-3ea0-46da-8b6b-a7c70d61fbec	TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7.svs	1cb06cd98f9e9c7687b2aee3e9772f49	2762599587	released
9bf65c40-afed-49c7-9a5f-60309cd687c6	TCGA-A6-6649-01Z-00-DX1.9439bce8-5715-4d76-a5d8-c6cbe1b79435.svs	7dd602f157f148a0971f8b64a016dd31	149890641	released
e5d2f743-a51f-4ba7-a23d-e87a069a7ee1	TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014.svs	6af6aff02d391d623297b524183d0c40	1766078883	released
ed5f8c30-29e3-4144-948b-b8658564f2d6	TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs	1f0cdb5cacd17c3dbf99f82ac41339dd	1228418533	released
103810e8-c721-4242-b729-270097a23469	TCGA-A6-5665-01Z-00-DX1.3ad2c249-d138-4037-a59b-4747ce2b789a.svs	73157eae5c5559cc546e475af4ad3af7	168725275	released
3fa4a855-ff4c-4f82-a248-229b4668d3ae	TCGA-A6-5664-01Z-00-DX1.622f6650-1926-4fa2-b42b-74122d9a68a4.svs	bdfe0950ef98b0e125b382d259482e60	294223087	released
</file>

<file path="tasks/stamp_extract_features/.gitignore">
*.svs
</file>

<file path="tasks/.gitignore">
*.parcel
*.partial
</file>

<file path="tasks/pytest.ini">
[pytest]
python_files = tests.py

markers =
    tool_invocation: mark test as a tool invocation
</file>

<file path=".cursorignore">
.legacy*
</file>

<file path=".python-version">
3.12
</file>

<file path=".github/workflows/docker.yml">
name: Build ToolArena runtime Docker image

on:
  workflow_dispatch:
  push:
    branches:
      - main

jobs:
  build-and-push-image:
    runs-on: ubuntu-latest

    permissions:
      contents: read
      packages: write
      attestations: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Log in to the Container registry
        uses: docker/login-action@65b78e6e13532edd9afa3aa52ac7964289d1a9c1
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@9ec57ed1fcdbf14dcef7dfbe97b2010124a938b7
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=ref,event=branch
            type=ref,event=tag
            type=raw,value=${{ github.ref_name }}-${{ github.run_number }}

      - name: Build and push Docker image
        id: push
        uses: docker/build-push-action@f2a1d5e99d037542a71f64918e516c093c6f3fc4
        with:
          context: .
          file: docker/runtime.Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
</file>

<file path="tasks/conch_extract_features/implementation.py">
def conch_extract_features(
    input_image: str = "/mount/input/TUM/TUM-TCGA-ACRLPPQE.tif",
) -> dict:
    """
    Perform feature extraction on an input image using CONCH.

    Args:
        input_image: Path to the input image

    Returns:
        dict with the following structure:
        {
          'features': list  # The feature vector extracted from the input image, as a list of floats
        }
    """
    import os

    import torch
    from conch.open_clip_custom.factory import create_model_from_pretrained
    from PIL import Image

    hf_token = os.environ.get("HF_TOKEN")
    model, preprocess = create_model_from_pretrained(
        model_cfg="conch_ViT-B-16",
        checkpoint_path="/workspace/CONCH/checkpoints/conch/pytorch_model.bin",
        device="cpu",
        hf_auth_token=hf_token,
    )
    image = Image.open(input_image).convert("RGB")
    image_tensor = preprocess(image).unsqueeze(0)

    with torch.inference_mode():
        image_embs = model.encode_image(
            image_tensor, proj_contrast=False, normalize=False
        )

    features = image_embs.cpu().numpy().tolist()[0]

    return {"features": features}
</file>

<file path="tasks/conch_extract_features/install.sh">
set -e

git clone https://github.com/mahmoodlab/CONCH /workspace/CONCH
cd /workspace/CONCH && git checkout 171f2be

pip install -e /workspace/CONCH
pip install huggingface_hub[cli] torch Pillow
mkdir -p /workspace/CONCH/checkpoints/conch
huggingface-cli download --local-dir /workspace/CONCH/checkpoints/conch MahmoodLab/CONCH pytorch_model.bin
</file>

<file path="tasks/conftest.py">
from collections.abc import Callable
from pathlib import Path

import pytest
from toolarena.definition import ToolInvocation
from toolarena.run import ToolRunResult

type ToolFixture = Callable[[ToolInvocation], ToolRunResult]
type InvocationFixture = Callable[[ToolFixture], ToolRunResult]


def pytest_addoption(parser):
    parser.addoption(
        "--candidate-impl",
        action="store",
        default=None,
        help="Path to the root folder with candidate implementations",
    )
    parser.addoption(
        "--skip-uncached",
        action="store_true",
        default=False,
        help="Skip uncached invocations",
    )


@pytest.fixture(scope="session")
def candidate_impl_dir(request) -> Path:
    """
    Returns the user-supplied path to a directory containing candidate tasks, e.g.:
      my_candidate_implementation/
        conch_extract_features/
          install.sh
          implementation.py
        some_other_task/
          install.sh
          implementation.py
    """
    path_str = request.config.getoption("--candidate-impl")
    if not path_str:
        path_str = Path(__file__).parent  # use tasks/ folder as default
    path = Path(path_str).resolve()
    if not path.is_dir():
        raise ValueError(f"Candidate impl directory does not exist: {path}")
    return path
</file>

<file path="tasks/utils.py">
"""Utilities for task tests."""

import inspect
from collections.abc import Callable, Mapping
from pathlib import Path

import pytest
from pytest_lazy_fixtures import lf
from toolarena import (
    ToolDefinition,
    ToolInvocation,
    ToolRunner,
    ToolRunResult,
)
from toolarena.utils import TASKS_DIR

type ToolFixture = Callable[[ToolInvocation], ToolRunResult]
type InvocationFixture = Callable[[ToolFixture], ToolRunResult]


def _invocation_fixture(
    tool_name: str,
    invocation_name: str,
    invocation: ToolInvocation,
    module: str,
    prefix: str | None = None,
) -> InvocationFixture:
    full_tool_name = f"{prefix}/{tool_name}" if prefix else tool_name

    def fixture(
        candidate_impl_dir: Path, request: pytest.FixtureRequest
    ) -> ToolRunResult:
        runner = ToolRunner.from_paths(
            task_file=TASKS_DIR / tool_name / "task.yaml",
            invocation=invocation,
            data_dir=TASKS_DIR / tool_name / "data",
            install_script=candidate_impl_dir / tool_name / "install.sh",
            code_implementation=candidate_impl_dir / tool_name / "implementation.py",
        )
        if (
            request.config.getoption("--skip-uncached", False)
            and not runner.is_cached()
        ):
            pytest.skip(
                f"Skipping uncached invocation {invocation_name} for {full_tool_name}"
            )
        return runner.run()

    fixture.__name__ = invocation_name
    fixture.__doc__ = f"Test invocation {invocation_name} for {full_tool_name}."
    fixture.__module__ = module

    return pytest.fixture(
        fixture,
        scope="module",
        params=[
            pytest.param(
                None,
                marks=[
                    pytest.mark.tool_invocation(
                        prefix=prefix, tool=tool_name, invocation=invocation_name
                    )
                ],
            )
        ],
        ids=[full_tool_name],
    )


def parametrize_invocation(
    *invocations: str | InvocationFixture,
) -> pytest.MarkDecorator:
    return pytest.mark.parametrize(
        "invocation",
        [lf(getattr(invocation, "__name__", invocation)) for invocation in invocations],
    )


def _get_fixtures(tool_name: str) -> Mapping[str, ToolFixture | InvocationFixture]:
    module = f"tasks.{tool_name}.tests"
    fixtures = {}
    definition = ToolDefinition.from_yaml(TASKS_DIR / tool_name / "task.yaml")
    for invocation_name, invocation in definition.test_invocations.items():
        fixtures[invocation_name] = _invocation_fixture(
            tool_name, invocation_name, invocation, module=module
        )
    return fixtures


def initialize() -> None:
    """Initialize the fixtures for the current test module.

    This function should be called from each test module, where the name of the test file is `<tool_name>/tests.py`.
    It will populate the global namespace with the fixtures for the tool.
    """
    frame = inspect.currentframe()
    try:
        caller_frame = frame.f_back
        file_path = Path(caller_frame.f_code.co_filename)
        assert file_path.name == "tests.py", (
            "This function should be called from a test file"
        )
        tool_name = file_path.parent.name
        caller_frame.f_globals.update(_get_fixtures(tool_name))
    finally:
        del frame  # Break reference cycle
</file>

<file path=".dockerignore">
/tasks
/data
/papers
/.legacy
/.cursor

/.env
</file>

<file path=".gitignore">
.DS_Store
/runs

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/



/workspace
/workspaces


# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/
</file>

<file path="tasks/conch_extract_features/task.yaml">
name: conch_extract_features
repo:
  name: CONCH
  url: "https://github.com/mahmoodlab/CONCH"
  commit: 171f2be
  env:
    HF_TOKEN: "${env:HF_TOKEN}"  # required for downloading models
papers:
  lu2024conch: "https://www.nature.com/articles/s41591-024-02856-4"
category: pathology
description: Perform feature extraction on an input image using CONCH.
arguments:
  input_image:
    description: Path to the input image
    type: str
returns:
  features:
    description: The feature vector extracted from the input image, as a list of floats
    type: list
example:
  arguments:
    input_image: /mount/input/TUM-TCGA-ACRLPPQE.tif
  mount:
    TUM-TCGA-ACRLPPQE.tif: TUM-TCGA-ACRLPPQE.tif
test_invocations:
  tif:
    arguments:
      input_image: /mount/input/MUC/MUC-TCGA-ACCPKIPN.tif
    mount:
      MUC-TCGA-ACCPKIPN.tif: MUC/MUC-TCGA-ACCPKIPN.tif
  png:
    arguments:
      input_image: /mount/input/TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png
    mount:
      TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png: TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png
  jpg:
    arguments:
      input_image: /mount/input/TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg
    mount:
      TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg: TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg
</file>

<file path="tasks/conch_extract_features/tests.py">
from pathlib import Path

import numpy as np
import pytest
from pytest_lazy_fixtures import lf
from tasks.utils import initialize, parametrize_invocation
from toolarena.run import ToolRunResult

initialize()


@parametrize_invocation("tif", "png", "jpg")
def test_status(invocation: ToolRunResult):
    assert invocation.status == "success"


@parametrize_invocation("tif", "png", "jpg")
def test_shape_and_type(invocation: ToolRunResult):
    assert "features" in invocation.result
    features = invocation.result["features"]
    assert isinstance(features, list)
    assert len(features) == 512
    assert all(isinstance(feature, float) for feature in features)


@pytest.mark.parametrize(
    "invocation,expected_features_file",
    [
        (lf(test_case), Path(__file__).parent.joinpath("data", "tests", filename))
        for (test_case, filename) in {
            "tif": "conch_MUC-TCGA-ACCPKIPN.tif.npy",
            "png": "conch_TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png.npy",
            "jpg": "conch_TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg.npy",
        }.items()
    ],
)
def test_feature_values(invocation: ToolRunResult, expected_features_file: Path):
    np.testing.assert_allclose(
        np.array(invocation.result["features"], dtype=np.float32),
        np.load(expected_features_file).squeeze(),
        atol=1e-3,
    )
</file>

<file path="toolarena/__init__.py">
from toolarena.definition import Repository, ToolDefinition, ToolInvocation
from toolarena.run import ToolImplementation, ToolRunner, ToolRunResult
from toolarena.utils import RUNS_DIR
</file>

<file path="toolarena/__main__.py">
from pathlib import Path
from typing import Annotated

import typer
from loguru import logger

from toolarena.definition import ToolDefinition, ToolInvocation
from toolarena.run import run_tool
from toolarena.utils import RUNS_DIR, TASKS_DIR

app = typer.Typer()


@app.command()
def signature(
    name: Annotated[str, typer.Argument(help="The name of the tool")],
) -> None:
    """Print the signature of a tool."""
    definition = ToolDefinition.from_yaml(TASKS_DIR / name / "task.yaml")
    print(definition.python_signature)


@app.command()
def generate(name: Annotated[str, typer.Argument(help="The name of the tool")]) -> None:
    """Generate the starting files for a new tool."""
    task_dir = TASKS_DIR / name
    definition_path = task_dir / "task.yaml"
    if not definition_path.exists():
        raise typer.Abort(f"Task definition {definition_path} does not exist")
    definition = ToolDefinition.from_yaml(definition_path)

    code_file = task_dir / "implementation.py"
    install_script = task_dir / "install.sh"
    tests_file = task_dir / "tests.py"

    if not code_file.exists():
        code_file.write_text(definition.python_signature)
        logger.info(f"Created {code_file}")

    if not install_script.exists():
        install_script.write_text(
            f"#! /bin/bash\n"
            f"{definition.repo.git_clone_command}\n\n"
            f"# Insert commands here to install dependencies and setup the environment...\n"
        )
        logger.info(f"Created {install_script}")
    if not tests_file.exists():
        tests_file.write_text("import pytest\n")
        logger.info(f"Created {tests_file}")
    print("Done!")


@app.command()
def run(
    name: Annotated[str, typer.Argument(help="The name of the tool")],
    invocation: Annotated[
        str | None,
        typer.Argument(
            help="The invocation to run (optional, default run all invocations)"
        ),
    ] = None,
    implementation: Annotated[
        Path | None,
        typer.Argument(
            help="Path to the folder where the implementation is stored (optional, default use the implementation in the task directory). This folder should contain an install.sh script and a implementation.py file."
        ),
    ] = None,
    cache: Annotated[
        Path | None,
        typer.Option(
            help="The root directory for caching tool runs (optional, default use the default cache root)"
        ),
    ] = RUNS_DIR,
) -> None:
    """Run a tool."""
    task_dir = TASKS_DIR / name
    task_file = task_dir / "task.yaml"
    definition = ToolDefinition.from_yaml(task_file)

    def _run(invocation_name: str, invocation: ToolInvocation) -> None:
        logger.info(f"Running {invocation_name} for {name}")
        result = run_tool(
            task_file=task_file,
            install_script=(implementation or task_dir) / "install.sh",
            code_implementation=(implementation or task_dir) / "implementation.py",
            invocation=invocation,
            data_dir=task_dir / "data",
            cache_root=cache,
        )
        print(
            f"Tool {name} invocation {invocation_name} finished with status {result.status}"
        )
        print(result.result)

    for invocation_name, invocation in (
        [
            ("example", definition.example),
            *definition.test_invocations.items(),
        ]
        if not invocation
        else [
            (
                invocation,
                definition.example
                if invocation == "example"
                else definition.test_invocations[invocation],
            )
        ]
    ):
        _run(invocation_name, invocation)


app()
</file>

<file path="pyproject.toml">
[project]
name = "toolarena"
version = "0.1.0"
authors = [
    { name = "Georg WÃ¶lflein", email = "georg@woelflein.de" },
]
description = "Benchmark for LLM tool creation"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "docker>=7.1.0",
    "fastapi>=0.115.11",
    "httpx>=0.28.1",
    "loguru>=0.7.3",
    "mcp[cli]>=1.4.1",
    "numpy>=2.2.4",
    "pydantic-settings>=2.8.1",
    "pytest>=8.3.5",
    "pytest-lazy-fixtures>=1.1.2",
    "python-dotenv>=1.0.1",
    "pyyaml>=6.0.2",
    "tenacity>=9.0.0",
    "typer>=0.15.2",
    "uvicorn>=0.34.0",
]


[tool.ruff]
src = ["toolarena"]

[tool.ruff.lint]
select = ["E", "F", "I001"]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "auto"

[dependency-groups]
dev = [
    "ipykernel>=6.29.5",
    "types-docker>=7.1.0.20241229",
    "types-pyyaml>=6.0.12.20241230",
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]  # Ignore `F401` (import violations) in all `__init__.py` files
"*.py" = ["E501"]  # Ignore `E501` (line too long) in all files
"*.ipynb" = ["E501"]  # Ignore `E501` (line too long) in all notebooks
</file>

<file path="run_tool.ipynb">
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from toolarena import ToolDefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolDefinition(name='conch_extract_features', repo=Repository(name='CONCH', url='https://github.com/mahmoodlab/CONCH', branch=None, commit='171f2be', env={'HF_TOKEN': '${env:HF_TOKEN}'}), papers=['lu2024conch'], category='pathology', description='Perform feature extraction on an input image using CONCH.', arguments={'input_image': ArgumentDefinition(description='Path to the input image', type='str')}, returns={'features': ArgumentDefinition(description='The feature vector extracted from the input image, as a list of floats', type='list')}, example=ToolInvocation(arguments={'input_image': '/mount/input/TUM-TCGA-ACRLPPQE.tif'}, mount={'TUM-TCGA-ACRLPPQE.tif': 'TUM-TCGA-ACRLPPQE.tif'}), test_invocations={'tif': ToolInvocation(arguments={'input_image': '/mount/input/MUC/MUC-TCGA-ACCPKIPN.tif'}, mount={'MUC-TCGA-ACCPKIPN.tif': 'MUC/MUC-TCGA-ACCPKIPN.tif'}), 'png': ToolInvocation(arguments={'input_image': '/mount/input/TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png'}, mount={'TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png': 'TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png'}), 'jpg': ToolInvocation(arguments={'input_image': '/mount/input/TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg'}, mount={'TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg': 'TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg'})}, note=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_dir = Path(\"tasks/conch_extract_features\")\n",
    "definition = ToolDefinition.from_yaml(task_dir / \"task.yaml\")\n",
    "definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-11 16:48:30.359\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtoolarena.run\u001b[0m:\u001b[36mbuild_tool\u001b[0m:\u001b[36m30\u001b[0m - \u001b[34m\u001b[1mBuilding tool image conch_extract_features in /tmp/tmpissx51w5 with environment {'HF_TOKEN': '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'}\u001b[0m\n",
      "\u001b[32m2025-04-11 16:48:30.367\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtoolarena.runtime\u001b[0m:\u001b[36mbuild_image\u001b[0m:\u001b[36m166\u001b[0m - \u001b[34m\u001b[1mBuilding image toolarena-tool:conch_extract_features using Docker BuildKit\u001b[0m\n",
      "Step 1/6 : FROM ghcr.io/georg-wolflein/toolarena:main\n",
      " ---> ee549a5650ea\n",
      "Step 2/6 : SHELL [\"/bin/bash\", \"-c\"]\n",
      " ---> Using cache\n",
      " ---> 4b29c90cf19a\n",
      "Step 3/6 : COPY install.sh .env ./\n",
      " ---> Using cache\n",
      " ---> 7ec87b30dba9\n",
      "Step 4/6 : RUN echo \">>>START INSTALL<<<\" &&     chmod +x install.sh &&     set -o allexport &&     . .env &&     set +o allexport &&     ./install.sh &&     rm -f .env\n",
      " ---> Using cache\n",
      " ---> c959ddb365e1\n",
      "Step 5/6 : RUN echo \">>>END INSTALL<<<\"\n",
      " ---> Using cache\n",
      " ---> 9c4235f5666f\n",
      "Step 6/6 : COPY task.yaml implementation.py ./\n",
      " ---> Using cache\n",
      " ---> e07ef518a486\n",
      "[Warning] One or more build-args [DOCKER_BUILDKIT] were not consumed\n",
      "Successfully built e07ef518a486\n",
      "Successfully tagged toolarena-tool:conch_extract_features\n",
      "\u001b[32m2025-04-11 16:48:30.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtoolarena.runtime\u001b[0m:\u001b[36mbuild_image\u001b[0m:\u001b[36m195\u001b[0m - \u001b[1mBuilt image toolarena-tool:conch_extract_features using Docker BuildKit\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolImplementation(definition=ToolDefinition(name='conch_extract_features', repo=Repository(name='CONCH', url='https://github.com/mahmoodlab/CONCH', branch=None, commit='171f2be', env={'HF_TOKEN': '${env:HF_TOKEN}'}), papers=['lu2024conch'], category='pathology', description='Perform feature extraction on an input image using CONCH.', arguments={'input_image': ArgumentDefinition(description='Path to the input image', type='str')}, returns={'features': ArgumentDefinition(description='The feature vector extracted from the input image, as a list of floats', type='list')}, example=ToolInvocation(arguments={'input_image': '/mount/input/TUM-TCGA-ACRLPPQE.tif'}, mount={'TUM-TCGA-ACRLPPQE.tif': 'TUM-TCGA-ACRLPPQE.tif'}), test_invocations={'tif': ToolInvocation(arguments={'input_image': '/mount/input/MUC/MUC-TCGA-ACCPKIPN.tif'}, mount={'MUC-TCGA-ACCPKIPN.tif': 'MUC/MUC-TCGA-ACCPKIPN.tif'}), 'png': ToolInvocation(arguments={'input_image': '/mount/input/TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png'}, mount={'TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png': 'TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.png'}), 'jpg': ToolInvocation(arguments={'input_image': '/mount/input/TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg'}, mount={'TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg': 'TCGA-BRCA_patch_TCGA-BH-A0DE-01Z-00-DX1.64A0340A-8146-48E8-AAF7-4035988B9152.jpg'})}, note=None), image=<Image: 'toolarena-tool:conch_extract_features'>, install_script='set -e\\n\\ngit clone https://github.com/mahmoodlab/CONCH /workspace/CONCH\\ncd /workspace/CONCH && git checkout 171f2be\\n\\npip install -e /workspace/CONCH\\npip install huggingface_hub[cli] torch Pillow\\nmkdir -p /workspace/CONCH/checkpoints/conch\\nhuggingface-cli download --local-dir /workspace/CONCH/checkpoints/conch MahmoodLab/CONCH pytorch_model.bin', code_implementation='def conch_extract_features(\\n    input_image: str = \"/mount/input/TUM/TUM-TCGA-ACRLPPQE.tif\",\\n) -> dict:\\n    \"\"\"\\n    Perform feature extraction on an input image using CONCH.\\n\\n    Args:\\n        input_image: Path to the input image\\n\\n    Returns:\\n        dict with the following structure:\\n        {\\n          \\'features\\': list  # The feature vector extracted from the input image, as a list of floats\\n        }\\n    \"\"\"\\n    import os\\n\\n    import torch\\n    from conch.open_clip_custom.factory import create_model_from_pretrained\\n    from PIL import Image\\n\\n    hf_token = os.environ.get(\"HF_TOKEN\")\\n    model, preprocess = create_model_from_pretrained(\\n        model_cfg=\"conch_ViT-B-16\",\\n        checkpoint_path=\"/workspace/CONCH/checkpoints/conch/pytorch_model.bin\",\\n        device=\"cpu\",\\n        hf_auth_token=hf_token,\\n    )\\n    image = Image.open(input_image).convert(\"RGB\")\\n    image_tensor = preprocess(image).unsqueeze(0)\\n\\n    with torch.inference_mode():\\n        image_embs = model.encode_image(\\n            image_tensor, proj_contrast=False, normalize=False\\n        )\\n\\n    features = image_embs.cpu().numpy().tolist()[0]\\n\\n    return {\"features\": features}\\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implementation = definition.build(\n",
    "    install_script=task_dir.joinpath(\"install.sh\").read_text(),\n",
    "    code_implementation=task_dir.joinpath(\"implementation.py\").read_text(),\n",
    ")\n",
    "implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-11 16:48:32.638\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtoolarena.run\u001b[0m:\u001b[36mread_cache\u001b[0m:\u001b[36m151\u001b[0m - \u001b[34m\u001b[1mRetrieving cached result for conch_extract_features at /mnt/bulk-uranus/gwoelflein/toolmaker/ToolArena/runs/conch_extract_features/b291a83adcfe214233a258b082a5cd8343119d2b2ecc13b2cb21ff229384cb82/result.json\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolRunResult(return_code=0, result={'features': [-2.1127400398254395, -0.12263921648263931, 0.0685218796133995, -1.8740019798278809, -1.9656271934509277, 1.1370997428894043, -0.40518924593925476, 1.1796488761901855, 0.5420725345611572, -1.247820258140564, 0.47097936272621155, 0.3854270875453949, -1.17879319190979, -1.0552204847335815, 0.5135803818702698, -1.5071587562561035, -0.11000212281942368, -1.772449016571045, -0.4886792302131653, -0.1738370954990387, 1.1974680423736572, -0.4748295247554779, -1.4243816137313843, -1.1926443576812744, 0.8477097153663635, -0.5402045845985413, -0.7401966452598572, -0.3739851713180542, 1.0221353769302368, -0.18208067119121552, -0.17498990893363953, -0.10263236612081528, -0.41960206627845764, 0.9942117929458618, -0.4005025029182434, -1.9455294609069824, -0.3738819360733032, 1.8011070489883423, -0.4269024133682251, 0.9055567979812622, -0.19818878173828125, 1.8628121614456177, -0.8676360845565796, -0.5792005658149719, 0.23494312167167664, -0.4399369955062866, -0.8914738297462463, 0.4246498644351959, 0.45010194182395935, -0.949687659740448, -0.6340894103050232, -1.7790141105651855, 1.086661696434021, 1.5564035177230835, 0.7121902108192444, 0.2671056389808655, -0.6461109519004822, 2.0303871631622314, -0.45860573649406433, -1.105893850326538, -1.4350507259368896, 0.5461419224739075, -0.2072954922914505, 0.20811855792999268, -0.9946370720863342, -0.888763964176178, -0.12125217169523239, 1.594773292541504, 0.6964996457099915, -0.3370177447795868, 1.5387420654296875, -0.5828816890716553, 1.7600055932998657, 0.3816165626049042, -1.3971320390701294, -0.34682193398475647, 1.340327262878418, -0.5749272704124451, -0.8339519500732422, -0.8677642941474915, 1.1742219924926758, -0.6178669333457947, -1.8901524543762207, -0.6055429577827454, 1.1776419878005981, 0.24642963707447052, 0.7621563076972961, 0.6447954773902893, 0.023456864058971405, 0.0798557847738266, 0.08459288626909256, 0.41368216276168823, -0.5126694440841675, -2.1684982776641846, -0.5790107846260071, -1.6077802181243896, 0.3862546980381012, 2.5168919563293457, -0.36193081736564636, -0.9694597721099854, 0.04726879298686981, -0.14535290002822876, -0.4103284478187561, 0.36642780900001526, 0.35545647144317627, 0.2870497703552246, 0.8562481999397278, 1.5281554460525513, -0.5295273065567017, 0.6616927981376648, 1.459641456604004, -0.695374608039856, 0.995898962020874, 2.417553424835205, -0.946090817451477, 0.4666258692741394, -0.4054388403892517, -0.30135655403137207, -1.3586267232894897, 0.5148051977157593, -0.520057201385498, 0.7837904691696167, 0.98557448387146, -1.699694275856018, 1.0111685991287231, 0.631319522857666, -0.8176481127738953, 0.8980860114097595, -0.6938620805740356, -0.8997018933296204, 0.13381484150886536, -1.1070107221603394, 0.07617177814245224, -1.6112525463104248, 1.3328709602355957, -0.3373289108276367, 0.025970857590436935, -0.17746920883655548, -0.3698746860027313, -0.5855363011360168, 1.8030062913894653, 0.008121883496642113, 0.08454171568155289, -2.220322847366333, 0.23976528644561768, -0.1444893330335617, -1.7768594026565552, 0.052290380001068115, -1.1959824562072754, -0.030040442943572998, -1.0742948055267334, -0.17128309607505798, 0.568590521812439, 0.6697315573692322, -0.6294746398925781, -1.3766087293624878, 0.5423433184623718, 0.9529001116752625, 1.5995160341262817, -1.5740805864334106, 0.23667854070663452, -0.580564022064209, 1.2863484621047974, -0.2517782151699066, 0.5810027122497559, -0.12903980910778046, 0.3725316822528839, 0.17329201102256775, 0.8346598744392395, 0.45641016960144043, -0.12191607058048248, -0.39315807819366455, -0.8360434174537659, -0.46653106808662415, -0.5969462394714355, 0.10560598969459534, -0.5903635621070862, 0.888523519039154, -0.40981295704841614, -0.4897501468658447, -0.508994460105896, 0.29809215664863586, -0.013483935967087746, 0.45030587911605835, 0.5336653590202332, 0.5351365208625793, -0.03218692168593407, -2.604607343673706, 0.44395771622657776, -1.2267965078353882, 0.9685004353523254, 0.806376039981842, -1.1097357273101807, -0.1314336210489273, -1.6072111129760742, -0.6290622353553772, 1.0608551502227783, -0.6209108233451843, 0.6463690996170044, 0.08977455645799637, 1.4068927764892578, 1.0999209880828857, 0.9109025001525879, -1.3391414880752563, 1.366971731185913, -0.9251993298530579, -1.1930806636810303, -0.21632704138755798, 0.08647046238183975, -0.8530614972114563, 0.3617434501647949, -0.3188946545124054, -2.154644250869751, 2.0915307998657227, -0.32985594868659973, 0.3534174859523773, -1.3289541006088257, 1.6099066734313965, -0.5497509241104126, -1.3177348375320435, 0.8036413788795471, 1.5054130554199219, 0.3363783061504364, -0.10205695033073425, 1.461928129196167, 2.2662265300750732, -0.4416219890117645, -1.075027585029602, -0.948237955570221, 0.8508857488632202, 1.1896194219589233, -0.7281438708305359, 1.4475356340408325, 1.3228330612182617, -0.6287024021148682, -1.7939300537109375, -0.03615706041455269, 0.7702271938323975, 0.5363504886627197, 0.18340080976486206, -0.8068508505821228, 0.8736308217048645, 1.1763619184494019, 0.7675041556358337, 1.9911577701568604, -1.3993498086929321, -1.3898974657058716, -0.9780176877975464, 2.9386096000671387, 0.7256223559379578, 1.1336008310317993, -0.8126580119132996, 0.7804833054542542, 1.100862741470337, 0.33088219165802, -0.4315638542175293, -0.22684769332408905, -0.7757591009140015, -0.41302546858787537, -1.805909276008606, -0.4990710914134979, 0.020856866613030434, 1.396397590637207, -0.007911496795713902, -0.3541952073574066, -0.16530311107635498, 0.5816650390625, -0.2825315594673157, -0.8106838464736938, -0.24950332939624786, 1.214702844619751, 0.04077818989753723, -0.539509654045105, -0.2729262411594391, 1.0379596948623657, 0.25245407223701477, -0.20171013474464417, -1.6022851467132568, 0.7960978746414185, -1.1209503412246704, 0.1873376965522766, -0.14933563768863678, -0.9092612266540527, 0.8761765956878662, -1.044633150100708, 0.006002683658152819, 1.4318925142288208, 1.7015010118484497, -2.026603937149048, -0.7897246479988098, 1.8801921606063843, 0.031046126037836075, -0.2557922899723053, 1.8243273496627808, 1.3162842988967896, 1.51204252243042, 1.2254434823989868, -0.7506608963012695, 1.1837401390075684, -0.4541606307029724, -0.286122590303421, 0.5794042348861694, 0.0677197277545929, -1.4062901735305786, -0.18902650475502014, -0.3337254822254181, -0.755236029624939, -1.6781904697418213, 0.03396906703710556, 0.4307580888271332, 0.3727640211582184, 1.4828025102615356, -0.08497797697782516, -0.7001469135284424, 0.2990296185016632, 0.5412245988845825, 0.07679238170385361, -1.0837117433547974, 1.3780832290649414, -1.319157361984253, 0.243717223405838, -0.972494900226593, 0.6269028782844543, 1.6196686029434204, -1.8019243478775024, 0.5498003363609314, 0.4231836497783661, -0.8057507276535034, 0.9911515712738037, -0.8633517026901245, -0.0875692293047905, 1.2676928043365479, -1.7720282077789307, -0.20047122240066528, 0.7778502106666565, 1.4270533323287964, -0.20037920773029327, 0.2933766543865204, 0.6946794986724854, 0.5005806088447571, 0.3705303966999054, 0.31981360912323, 0.7411486506462097, 1.93812894821167, 0.09788598120212555, -0.029095860198140144, 0.3214947283267975, -1.10774564743042, -0.7501954436302185, 1.1485697031021118, 0.34170079231262207, 0.37919628620147705, 1.4092329740524292, -1.7368073463439941, -0.634616494178772, 1.6489858627319336, -0.33500877022743225, 2.068516492843628, 0.4408610761165619, -0.5653950572013855, 0.7084037065505981, -1.8646087646484375, 0.16337822377681732, -0.7124910950660706, -0.42308253049850464, -1.1786540746688843, -1.6744132041931152, -1.0306715965270996, 0.6962575912475586, -1.343639850616455, -1.137009859085083, -0.18853624165058136, 0.1954926699399948, -1.0029124021530151, -0.9015837907791138, -0.12659227848052979, -0.6895448565483093, 0.6999750137329102, 1.0353461503982544, 1.1248024702072144, 0.4237475097179413, 2.2198596000671387, -0.5626038312911987, 0.45350077748298645, 0.37216895818710327, 0.041213493794202805, -0.41155579686164856, 0.766726016998291, 0.2771798074245453, -0.33607783913612366, -1.6420646905899048, 1.4730377197265625, -2.0687453746795654, -1.1252334117889404, -2.1114869117736816, 0.643632173538208, -1.4788888692855835, 0.7796543836593628, 0.923126757144928, 0.806684672832489, -0.5115380883216858, 1.9023507833480835, -0.48203471302986145, 0.19721560180187225, 1.4913195371627808, -0.007094535976648331, 0.42980602383613586, 1.1410778760910034, 1.6593997478485107, 0.4819610118865967, 0.4625808894634247, -0.6107710003852844, -1.1023344993591309, -0.12376898527145386, 0.20440076291561127, 1.6595557928085327, -0.637205958366394, -0.2558084726333618, -1.2668288946151733, -1.230447769165039, 0.08219815045595169, -0.2718442380428314, 0.3025442659854889, -0.34246599674224854, 0.2614329159259796, -1.1031932830810547, 0.31354472041130066, 0.6533303260803223, 0.5791955590248108, 2.2070724964141846, -0.31738582253456116, -0.034103766083717346, 1.1338255405426025, -1.6849416494369507, 0.11173619329929352, -0.5617504119873047, -0.15346841514110565, 0.7596341371536255, 0.4701591432094574, 0.5213397741317749, 0.16666164994239807, 1.843830943107605, 0.09740390628576279, -0.973726212978363, -0.6190483570098877, 0.06552717834711075, -0.8613002896308899, -0.34658923745155334, -1.0837959051132202, -0.9368041157722473, 0.4935935437679291, -1.086165428161621, -1.6114896535873413, 1.3729194402694702, -0.11139082908630371, 0.20995421707630157, -1.6239190101623535, 0.9167736768722534, 1.0747714042663574, 1.2453590631484985, 0.33693575859069824, 0.9324861764907837, -0.9132102727890015, -1.6941510438919067, 0.7738766074180603, -0.839667558670044, 0.21236982941627502, 0.5438804030418396, 1.1785914897918701, -0.1152830719947815, 0.546647846698761, 0.0697561725974083, 1.3311378955841064, 1.1044740676879883, -0.6477696299552917, -0.39711400866508484, 0.3940083980560303, -0.38845980167388916, 0.08860462158918381, -1.298984408378601, 0.2966553866863251, -0.198678120970726, 1.7838118076324463, 0.17659905552864075, -1.3147038221359253, 1.4586845636367798, 0.32011228799819946, 0.252163290977478, -2.0187153816223145, 0.9403297901153564, -0.9981827139854431, 0.28361207246780396, 1.1338744163513184, -0.05656225606799126, -0.6684452295303345, -1.1175973415374756, -0.929390013217926, -0.17048463225364685, -1.0614005327224731, 1.26939857006073, -2.0326502323150635, -0.060663241893053055, 1.1542168855667114, 2.053936243057251, 1.7952971458435059, -2.331071615219116, 0.6170142292976379, -1.9017574787139893, -0.36467140913009644, 1.6776677370071411, 0.11993927508592606, -0.5004714727401733]}, stdout='/usr/local/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\\n', output_dir=PosixPath('/mnt/bulk-uranus/gwoelflein/toolmaker/ToolArena/runs/conch_extract_features/b291a83adcfe214233a258b082a5cd8343119d2b2ecc13b2cb21ff229384cb82/output'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = implementation.run(invocation=definition.example, data_dir=task_dir / \"data\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</file>

<file path="docker/runtime.Dockerfile">
# This image is published to ghcr.io/georg-wolflein/toolarena-runtime
FROM python:3.12

WORKDIR /toolarena

COPY pyproject.toml uv.lock ./

RUN python -m pip install uv && \
    uv venv && \
    uv sync

COPY toolarena ./toolarena

RUN python -m pip install uv && \
    uv venv && \
    uv sync

RUN mkdir -p /workspace
WORKDIR /workspace

COPY docker/subprocess_utils.py /toolarena/subprocess_utils.py
COPY docker/function_runner.py /toolarena/function_runner.py

# For backwards compatibility with the old toolmaker runtime
RUN mkdir -p /toolmaker && ln -s /toolarena/subprocess_utils.py /toolmaker/subprocess_utils.py

VOLUME /mount/input
VOLUME /mount/output

ENV WORKSPACE_DIR=/workspace
ENV HOST=0.0.0.0
ENV PORT=8000

CMD /toolarena/.venv/bin/uvicorn --app-dir /toolarena --host ${HOST} --port ${PORT} toolarena.server:app
</file>

<file path="docker/tool.Dockerfile">
FROM ghcr.io/georg-wolflein/toolarena:main

# Use bash (so we can use the "." command)
SHELL ["/bin/bash", "-c"]

COPY install.sh .env ./

# Run the install script (using environment variables)
# We prefix the command with ">>>START INSTALL<<<" and ">>>END INSTALL<<<" to make it easier to extract the output of the install script from the logs
RUN echo ">>>START INSTALL<<<" && \
    chmod +x install.sh && \
    set -o allexport && \
    . .env && \
    set +o allexport && \
    ./install.sh && \
    rm -f .env
RUN echo ">>>END INSTALL<<<"

COPY task.yaml implementation.py ./
</file>

<file path="toolarena/definition.py">
from __future__ import annotations

from collections.abc import Mapping, Sequence
from pathlib import Path
from typing import TYPE_CHECKING, Literal, Self

import yaml
from pydantic import BaseModel, Field, create_model
from pydantic_settings import BaseSettings

from toolarena.utils import substitute_env_vars

if TYPE_CHECKING:
    from toolarena.run import ToolImplementation

type ArgumentTypeName = Literal["str", "int", "float", "bool", "list", "dict"]
type ArgumentType = str | int | float | bool | list | dict | None

argument_type_map: Mapping[ArgumentTypeName, type] = {
    "str": str,
    "int": int,
    "float": float,
    "bool": bool,
    "list": list,
    "dict": dict,
}


class ArgumentDefinition(BaseModel):
    description: str
    type: ArgumentTypeName


class ToolInvocation(BaseModel):
    arguments: Mapping[str, ArgumentType]
    mount: Mapping[str, str] = Field(default_factory=dict)


class Repository(BaseModel):
    name: str
    url: str
    branch: str | None = None
    commit: str | None = None
    env: Mapping[str, str] = Field(default_factory=dict)

    @property
    def name_without_owner(self) -> str:
        return self.name.split("/")[-1]

    def info(self) -> str:
        s = f"{self.name} from {self.url}"
        if self.commit:
            s += f" (at commit: {self.commit})"
        if self.branch:
            s += f" (at branch: {self.branch})"
        return s

    @property
    def git_clone_command(self) -> str:
        repo_dir = f"/workspace/{self.name}"
        cmd = f"git clone {self.url} {repo_dir}"
        if self.branch or self.commit:
            cmd += f"\ncd {repo_dir}"
        if self.branch:
            cmd += f" && git checkout {self.branch}"
        if self.commit:
            cmd += f" && git checkout {self.commit}"
        return cmd

    def resolve_env(self, env: Mapping[str, str] | None = None) -> Mapping[str, str]:
        return {k: substitute_env_vars(v, env) for k, v in self.env.items()}


class ToolDefinition(BaseSettings):
    name: str
    repo: Repository
    papers: Mapping[str, str]
    category: str
    description: str
    arguments: Mapping[str, ArgumentDefinition]
    returns: Mapping[str, ArgumentDefinition]
    example: ToolInvocation
    test_invocations: Mapping[str, ToolInvocation] = Field(default_factory=dict)
    note: str | None = (
        None  # additional information about this task (will not be shown to the model)
    )

    @classmethod
    def from_yaml(cls, path: Path | str) -> Self:
        return cls(**yaml.safe_load(open(path, "r")))

    def _arg_str(self) -> str:
        return ", ".join(
            f"{name}: {arg.type} = {self.example.arguments[name]!r}"
            for name, arg in self.arguments.items()
        )

    def description_of_returns(self) -> str:
        if self.returns:
            return f"""dict with the following structure:
{{
{"\n".join(f"  {key!r}: {arg.type}  # {arg.description}" for key, arg in self.returns.items())}
}}"""
        return "empty dict"

    @property
    def python_signature(self) -> str:
        indent = " " * 4
        return f"""def {self.name}({self._arg_str()}) -> dict:
{indent}\"\"\"
{indent}{self.description.replace("\n", f"\n{indent}")}
{indent}
{indent}Args:
{"\n".join(f"{indent}    {name}: {arg.description}" for name, arg in self.arguments.items())}
{indent}
{indent}Returns:
{indent}    {self.description_of_returns().replace("\n", f"\n{indent}    ")}
{indent}\"\"\"
"""

    def args_to_pydantic(self, name: str = "ToolCall") -> BaseModel:
        return create_model(  # type: ignore
            name,
            **{
                k: (argument_type_map[v.type], Field(description=v.description))
                for k, v in self.arguments.items()
            },
        )

    def build(
        self, install_script: str, code_implementation: str
    ) -> ToolImplementation:
        from toolarena.run import build_tool

        return build_tool(
            definition=self,
            install_script=install_script,
            code_implementation=code_implementation,
        )
</file>

<file path="toolarena/utils.py">
import os
import re
import shutil
from collections.abc import Container, Mapping
from pathlib import Path

from loguru import logger

ROOT_DIR = Path(__file__).parent.parent
RUNS_DIR = ROOT_DIR / "runs"
TASKS_DIR = ROOT_DIR / "tasks"


def join_paths(parent: os.PathLike | str, *children: os.PathLike | str) -> Path:
    """Join paths and ensure that the resulting path is relative to the parent."""
    path = Path(parent)
    for child in children:
        path = path / Path(child)
    if not path.resolve().is_relative_to(Path(parent).resolve()):
        raise ValueError(f"Path {path} is not relative to {parent}")

    return path


def chown_dir_using_docker(
    dir: os.PathLike | str, uid: int = os.getuid(), gid: int = os.getgid()
) -> None:
    import docker

    client = docker.from_env()
    logger.debug(f"Chowning directory {dir} to {uid}:{gid} using docker")
    client.containers.run(
        "busybox",
        ["chown", "-R", f"{uid}:{gid}", "/mnt/mydir"],
        volumes={str(Path(dir).resolve()): {"bind": "/mnt/mydir", "mode": "rw"}},
        auto_remove=True,
    )


def rmdir(dir: os.PathLike | str) -> None:
    try:
        if Path(dir).exists():
            shutil.rmtree(dir)
    except PermissionError:
        logger.warning(
            f"Failed to remove directory {dir}. "
            "This may happen because docker wrote files with permissions that are not writable by the current user. "
            "We will try to chown the directory to the current user."
        )
        chown_dir_using_docker(dir)
        shutil.rmtree(dir)
    logger.debug(f"Removed directory {dir}.")


ENV_VAR_SUBSTITUTION_PATTERN = re.compile(
    r"(?P<full>\$\{env:(?P<var>[A-Za-z_][A-Za-z0-9_]*)\})"
)


def substitute_env_vars(
    s: str,
    env: Mapping[str, str] | None = None,
    allowed: Container[str]
    | None = None,  # if set, only these variables will be substituted
) -> str:
    if env is None:
        env = os.environ

    def substitute(match: re.Match) -> str:
        var = match.group("var")
        if var not in env:
            logger.warning(
                f"Unable to perform environment variable substitution for {var}: not found in environment"
            )
            return match.group("full")
        if allowed is not None and var not in allowed:
            logger.warning(
                f"Unable to perform environment variable substitution for {var}: not in list of allowed environment variable substitutions ({allowed!r})"
            )
            return match.group("full")
        return env[var]

    return ENV_VAR_SUBSTITUTION_PATTERN.sub(substitute, s)
</file>

<file path="toolarena/run.py">
from __future__ import annotations

import hashlib
import tempfile
from pathlib import Path
from typing import Self

import dotenv
import yaml
from docker.models.images import Image
from loguru import logger
from pydantic import BaseModel, ConfigDict

from toolarena.definition import ToolDefinition, ToolInvocation
from toolarena.runtime import (
    DockerRuntimeClient,
    Mounts,
    ToolResult,
    ToolRunResult,
    build_image,
)
from toolarena.utils import RUNS_DIR


def build_tool(
    definition: ToolDefinition, install_script: str, code_implementation: str
) -> ToolImplementation:
    environment = definition.repo.resolve_env()
    with tempfile.TemporaryDirectory() as temp_dir:
        logger.debug(
            f"Building tool image {definition.name} in {temp_dir} with environment {environment}"
        )
        temp_dir_path = Path(temp_dir)
        temp_dir_path.joinpath("task.yaml").write_text(
            yaml.dump(definition.model_dump())
        )
        temp_dir_path.joinpath("install.sh").write_text(install_script)
        temp_dir_path.joinpath("implementation.py").write_text(code_implementation)
        temp_dir_path.joinpath(".env").touch()
        for key, value in environment.items():
            dotenv.set_key(temp_dir_path / ".env", key, value)
        image, logs = build_image(tag=definition.name, context=temp_dir_path)
        return ToolImplementation(
            definition=definition,
            image=image,
            install_script=install_script,
            code_implementation=code_implementation,
        )


class ToolImplementation(BaseModel):
    definition: ToolDefinition
    image: Image
    install_script: str
    code_implementation: str
    model_config = ConfigDict(arbitrary_types_allowed=True)

    def run(self, invocation: ToolInvocation, data_dir: Path) -> ToolResult:
        return ToolRunner(
            definition=self.definition,
            invocation=invocation,
            data_dir=data_dir,
            install_script=self.install_script,
            code_implementation=self.code_implementation,
        ).run(self.image)


class ToolRunner(BaseModel):
    definition: ToolDefinition
    invocation: ToolInvocation
    data_dir: Path
    install_script: str
    code_implementation: str
    _cache_root: Path = RUNS_DIR

    @classmethod
    def from_paths(
        cls,
        *,
        task_file: Path,
        invocation: ToolInvocation,
        data_dir: Path,
        install_script: Path,
        code_implementation: Path,
    ) -> Self:
        return cls(
            definition=ToolDefinition.from_yaml(task_file),
            invocation=invocation,
            data_dir=data_dir,
            install_script=install_script.read_text(),
            code_implementation=code_implementation.read_text(),
        )

    def hash(self) -> str:
        return hashlib.sha256(self.model_dump_json().encode("utf-8")).hexdigest()

    @property
    def run_dir(self) -> Path:
        return self._cache_root / self.definition.name / self.hash()

    @property
    def input_dir(self) -> Path:
        return self.run_dir / "input"

    @property
    def output_dir(self) -> Path:
        return self.run_dir / "output"

    @property
    def cache_file(self) -> Path:
        return self.run_dir / "result.json"

    def run_without_cache(self, image: Image | None = None) -> ToolResult:
        """Run tool without using cache. If image is not provided, build it."""
        if image is None:
            logger.debug("Image not provided, building it")
            image = build_tool(
                definition=self.definition,
                install_script=self.install_script,
                code_implementation=self.code_implementation,
            ).image
        mounts = Mounts(
            input=self.input_dir,
            output=self.output_dir,
            data_dir=self.data_dir,
            input_mapping=self.invocation.mount,
        )
        mounts.setup()
        client = DockerRuntimeClient.create(
            name=self.definition.name, image=image, mounts=mounts
        )
        return client.run(**self.invocation.arguments)

    def run(self, image: Image | None = None) -> ToolRunResult:
        """Build image and run tool, using cache if available."""
        if self.is_cached():
            return self.read_cache()
        result = self.run_without_cache(image)
        self.write_cache(result)
        return self.read_cache()

    def write_cache(self, result: ToolResult):
        self.run_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file.write_text(result.model_dump_json(indent=2))
        self.run_dir.joinpath("install.sh").write_text(self.install_script)
        self.run_dir.joinpath("implementation.py").write_text(self.code_implementation)
        self.run_dir.joinpath("task.yaml").write_text(
            yaml.dump(self.definition.model_dump())
        )

    def read_cache(self) -> ToolRunResult:
        logger.debug(
            f"Retrieving cached result for {self.definition.name} at {self.cache_file}"
        )
        return ToolRunResult(
            **ToolResult.model_validate_json(self.cache_file.read_text()).model_dump(),
            output_dir=self.output_dir,
        )

    def is_cached(self) -> bool:
        return self.cache_file.exists()
</file>

<file path="toolarena/server.py">
import asyncio
import json
import os
import shlex
from pathlib import Path
from uuid import uuid4

from fastapi import FastAPI
from loguru import logger

from toolarena.definition import ToolDefinition
from toolarena.types import ToolRunResult

RUNTIME_DIR = Path(os.getenv("WORKSPACE_DIR", "/workspace"))
TOOLARENA_DIR = Path(os.getenv("TOOLARENA_DIR", "/toolarena"))
IMPLEMENTATION_PATH = RUNTIME_DIR / "implementation.py"
TASK_DEFINITION_PATH = RUNTIME_DIR / "task.yaml"
FUNCTION_RUNNER_PATH = TOOLARENA_DIR / "function_runner.py"

app = FastAPI()

task_definition = ToolDefinition.from_yaml(TASK_DEFINITION_PATH)


@app.get("/info")
async def info():
    return task_definition


@app.get("/alive")
async def alive():
    return {"status": "ok"}


async def run(args: task_definition.args_to_pydantic()) -> ToolRunResult:  # type: ignore
    function_name = task_definition.name
    logger.info(f"Running {function_name} with args: {args}")
    id = uuid4()
    info_path = Path(f"/tmp/{id}_info.json")
    output_path = Path(f"/tmp/{id}_output.json")
    json.dump(
        {
            "path": str(IMPLEMENTATION_PATH.absolute()),
            "name": function_name,
            "args": args.model_dump(),  # type: ignore
            "output_path": str(output_path.absolute()),
        },
        open(info_path, "w"),
    )
    process = await asyncio.create_subprocess_shell(
        shlex.join(
            [
                "python",
                str(FUNCTION_RUNNER_PATH.absolute()),
                str(info_path.absolute()),
            ]
        ),
        stdin=asyncio.subprocess.DEVNULL,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.STDOUT,
        text=False,
        cwd="/workspace",
        env=os.environ,
        start_new_session=True,  # this is to make sure an error is thrown if the command attempts to read from stdin
    )
    stdout = b""
    while (chunk := await process.stdout.read(32)) != b"":  # type: ignore
        stdout += chunk
        try:
            print(chunk.decode("utf-8"), end="")
        except UnicodeDecodeError:
            print(chunk, end="")
    return_code = await process.wait()
    response = ToolRunResult(
        return_code=return_code,
        result=json.loads(output_path.read_text())["result"]
        if output_path.exists()
        else None,
        stdout=stdout.decode("utf-8"),
    )
    for file in (info_path, output_path):
        file.unlink(missing_ok=True)

    return response


app.post("/run", description=task_definition.description)(run)
app.post(f"/{task_definition.name}", description=task_definition.description)(run)
</file>

<file path="toolarena/runtime.py">
"""This is the client that communicates with the tool runtime running inside a Docker container."""

import itertools
import os
import re
import shutil
import sys
from collections.abc import Mapping
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Final, Iterator, Literal, Self, Sequence, cast

import docker
import httpx
import tenacity
from docker.errors import BuildError
from docker.errors import NotFound as DockerNotFoundError
from docker.models.containers import Container
from docker.models.images import Image
from docker.types import DeviceRequest, Mount
from docker.utils.json_stream import json_stream
from loguru import logger
from pydantic import BaseModel

from toolarena.definition import ArgumentType
from toolarena.utils import ROOT_DIR, join_paths, rmdir

type MountMapping = Mapping[str, str]  # host -> container


TOOL_DOCKERFILE: Final[Path] = ROOT_DIR / "docker" / "tool.Dockerfile"
DEFAULT_TOOL_IMAGE_NAME: Final[str] = "toolarena-tool"
DOCKER_CONTAINER_PORT: Final[str] = "8000/tcp"


class ToolResult(BaseModel):
    return_code: int
    result: Any
    stdout: str

    @property
    def status(self) -> Literal["success", "failure"]:
        return "success" if self.return_code == 0 else "failure"


class ToolRunResult(ToolResult):
    output_dir: Path


@dataclass(frozen=True, kw_only=True)
class HTTPToolClient:
    host: str
    port: int

    http_client = httpx.Client(timeout=None)

    @property
    def url(self) -> str:
        return f"http://{self.host}:{self.port}"

    def run(self, **kwargs: ArgumentType) -> ToolResult:
        response = self.http_client.post(f"{self.url}/run", json=kwargs)
        return ToolResult.model_validate_json(response.text)

    def is_alive(self) -> bool:
        try:
            response = self.http_client.get(f"{self.url}/alive")
            return (
                response.status_code == 200
                and response.json().get("status", None) == "ok"
            )
        except httpx.HTTPError:
            return False

    def wait_for_alive(self, timeout: float | None = 10.0) -> Self:
        logger.debug(f"Waiting for runtime client {self.name} to become ready")
        if timeout is None:
            timeout = float("inf")

        try:
            tenacity.retry(
                retry=tenacity.retry_if_result(False.__eq__),
                stop=tenacity.stop_after_delay(timeout),
                wait=tenacity.wait_fixed(1),
            )(self.is_alive)()
        except tenacity.RetryError:
            raise RuntimeError(
                f"Runtime client did not become ready after {timeout} seconds. You may want to inspect the container logs using `docker logs {self.name}`"
            )
        return self


def get_docker() -> docker.DockerClient:
    return docker.from_env(timeout=480)  # increase timeout to avoid timeout errors


@dataclass(frozen=True)
class Mounts:
    input: Path | None = None  # folder to mount as input
    output: Path | None = None  # folder to mount as output
    data_dir: Path | None = None  # folder to copy input data from
    input_mapping: MountMapping | None = None  # mapping of input data to mount

    def to_docker(self) -> list[Mount]:
        mounts = []
        if self.input is not None:
            mounts.append(
                Mount(
                    target="/mount/input",
                    source=str(Path(self.input).resolve()),
                    type="bind",
                    read_only=False,  # TODO: make read-only?
                )
            )
        if self.output is not None:
            mounts.append(
                Mount(
                    target="/mount/output",
                    source=str(Path(self.output).resolve()),
                    type="bind",
                    read_only=False,
                )
            )
        return mounts

    def setup(self) -> None:
        """Setup the input and output mounts by copying data."""
        if self.output:
            rmdir(self.output)
            self.output.mkdir(parents=True, exist_ok=True)

        if self.input:
            rmdir(self.input)
            self.input.mkdir(parents=True, exist_ok=True)
            if not self.input_mapping:
                logger.debug("Not creating any input mounts...")
                return
            if not self.data_dir:
                raise ValueError("data_dir is required")

            for src, dst in self.input_mapping.items():
                src_path = join_paths(self.data_dir, src)
                dst_path = join_paths(self.input, dst)
                logger.debug(f"Copying {src_path} to {dst_path}")
                if not src_path.exists():
                    raise FileNotFoundError(f"Input data not found: {src_path}")
                dst_path.parent.mkdir(parents=True, exist_ok=True)
                if src_path.is_dir():
                    shutil.copytree(src_path, dst_path)
                else:
                    shutil.copy(src_path, dst_path)


def build_image(
    repository: str = DEFAULT_TOOL_IMAGE_NAME,
    *,
    tag: str,
    context: Path | str,
    dockerfile: Path | str = TOOL_DOCKERFILE,
) -> tuple[Image, Iterator[Mapping[str, str]]]:
    """Build an image using Docker BuildKit via the low-level Docker API.

    The implementation follows the implementation of `DockerClient.images.build()`.
    This function streams the build output to the console while the build is running.
    """
    logger.debug(f"Building image {repository}:{tag} using Docker BuildKit")
    resp = get_docker().api.build(
        path=str(context),
        dockerfile=str(dockerfile),
        tag=f"{repository}:{tag}",
        buildargs={
            "DOCKER_BUILDKIT": "1",
            # "BUILDKIT_PROGRESS": "plain",
        },
    )

    if isinstance(resp, str):
        return get_docker().images.get(resp)
    last_event = None
    image_id = None
    internal_stream, result_stream = itertools.tee(json_stream(resp))
    for chunk in internal_stream:
        if "error" in chunk:
            logger.error(f"Build error: {chunk['error']}")
            raise BuildError(chunk["error"], result_stream)
        if "stream" in chunk:
            print(chunk["stream"], end="", file=sys.stderr)
            match = re.search(
                r"(^Successfully built |sha256:)([0-9a-f]+)$", chunk["stream"]
            )
            if match:
                image_id = match.group(2)
        last_event = chunk
    if image_id:
        logger.info(f"Built image {repository}:{tag} using Docker BuildKit")
        return get_docker().images.get(image_id), result_stream
    raise BuildError(last_event or "Unknown", result_stream)


@dataclass(frozen=True, kw_only=True)
class DockerRuntimeClient(HTTPToolClient):
    name: str  # the name of the container
    image: Image  # the docker image

    @classmethod
    def _get_host_port(cls, container: Container) -> int:
        container.reload()
        if not (ports := container.ports):
            raise RuntimeError("Container is not running")
        port = ports[DOCKER_CONTAINER_PORT][0]["HostPort"]
        return int(port.split("/")[0])  # may be "1234/tcp"

    @classmethod
    def _start_container(
        cls,
        image: Image,
        name: str,
        port: int | None = None,  # None lets SDK choose port
        mounts: Mounts | None = None,
        gpus: Sequence[str] | None = None,
        env: Mapping[str, str] = {},
    ) -> Container:
        device_requests = []
        if gpus is None:
            gpus = os.getenv("CUDA_VISIBLE_DEVICES", "").split(",")
        if gpus:
            device_requests.append(
                DeviceRequest(device_ids=gpus, capabilities=[["gpu"]])
            )
            if "CUDA_VISIBLE_DEVICES" not in env:
                env = dict(env)
                env["CUDA_VISIBLE_DEVICES"] = ",".join(str(i) for i in range(len(gpus)))
        logger.debug(f"Starting container with GPUs {gpus}")

        # Start a container with the supplied name
        container = get_docker().containers.run(
            image=image,
            name=name,
            detach=True,
            ports={DOCKER_CONTAINER_PORT: port},
            tty=True,
            mounts=(mounts or Mounts()).to_docker(),
            mem_limit="100g",
            shm_size="10g",
            device_requests=device_requests,
            environment=dict(env),
        )
        logger.info(
            f"Started runtime client {container.name} on port {cls._get_host_port(container)} from image {image.tags}"
        )
        return container

    @classmethod
    def create(
        cls,
        name: str,
        image: str | Image,
        port: int | None = None,
        timeout: float | None = 10.0,  # max wait time for the runtime to become ready
        mounts: Mounts | None = None,
        gpus: Sequence[str] | None = None,
        env: Mapping[str, str] = {},
    ) -> Self:
        """Create a new runtime client by building the image and starting the container."""
        client = get_docker()
        docker_image = cast(
            Image,
            image if isinstance(image, Image) else client.images.get(image),
        )

        try:
            container: Container = client.containers.get(name)
            logger.info(f"Found existing container {name}, removing it")
            container.remove(force=True)
        except DockerNotFoundError:
            pass

        container = cls._start_container(
            image=docker_image,
            name=name,
            mounts=mounts,
            gpus=gpus,
            env=env,
            port=port,
        )
        return cls(
            host="localhost",
            port=cls._get_host_port(
                container
            ),  # (can't use port directly because it may be None, but we want to use the port that was assigned)
            name=container.name,  # type: ignore
            image=docker_image,
        ).wait_for_alive(timeout=timeout)

    def stop(self):
        container = get_docker().containers.get(self.name)
        container.stop()
        container.remove(force=True)

    def __enter__(self) -> Self:
        return self

    def __exit__(self, exc_type, exc_value, traceback) -> None:
        self.stop()
</file>

</files>
